--- Cell 1 ---
Using device: cuda
Mounted at /content/drive
Google Drive mounted and folder '/content/drive/MyDrive/FINETUNE_RES' ensured.


--- Cell 3 ---

--- Cell 5 ---

--- Cell 7 ---

--- Cell 9 ---
Loading and processing dataset...

/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: 
The secret `HF_TOKEN` does not exist in your Colab secrets.
To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.
You will be able to reuse this secret in all of your notebooks.
Please note that authentication is recommended but still optional to access public models or datasets.
  warnings.warn(

README.md:   0%|          | 0.00/497 [00:00<?, ?B/s]
train.csv: 0.00B [00:00, ?B/s]
test.csv: 0.00B [00:00, ?B/s]
Generating train split:   0%|          | 0/472 [00:00<?, ? examples/s]
Generating test split:   0%|          | 0/49 [00:00<?, ? examples/s]
tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]
config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]
vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]
merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]
tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]
Map:   0%|          | 0/462 [00:00<?, ? examples/s]
Map:   0%|          | 0/5 [00:00<?, ? examples/s]
Map:   0%|          | 0/5 [00:00<?, ? examples/s]
Map:   0%|          | 0/462 [00:00<?, ? examples/s]
Map:   0%|          | 0/5 [00:00<?, ? examples/s]
Map:   0%|          | 0/5 [00:00<?, ? examples/s]

=== Running Baseline ===

model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]
generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]
`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.
Device set to use cuda:0

Baseline PPL: 68.46

--- Generating samples for Baseline ---

=== Running Full Fine-tune ===

<IPython.core.display.HTML object>
Device set to use cuda:0

Full FT PPL: 25.64

--- Generating samples for Full FT ---

=== Running LoRA (Rank=16) ===

/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
  warnings.warn(

Trainable: 294912 | %: 0.3587

<IPython.core.display.HTML object>
Device set to use cuda:0

LoRA (r=16) PPL: 49.00

--- Generating samples for LoRA (r=16) ---

=== Starting Ablation Study ===

=== Running LoRA (Rank=1) ===

/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
  warnings.warn(

Trainable: 18432 | %: 0.0225

<IPython.core.display.HTML object>
LoRA (r=1) PPL: 53.25

=== Running LoRA (Rank=8) ===

/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
  warnings.warn(

Trainable: 147456 | %: 0.1797

<IPython.core.display.HTML object>
LoRA (r=8) PPL: 53.61

=== Running LoRA (Rank=64) ===

/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
  warnings.warn(

Trainable: 1179648 | %: 1.4197

<IPython.core.display.HTML object>
LoRA (r=64) PPL: 53.53

Saved /content/drive/MyDrive/FINETUNE_RES/perplexity_ablation.csv
           Model  Rank        PPL  Trainable Params  Total Params    Params %
0       Baseline     0  68.463905                 0      81912576    0.000000
1        Full FT     0  25.640654          81912576      81912576  100.000000
2           LoRA    16  48.997121            294912      82207488    0.358741
3  LoRA Ablation     1  53.245688             18432      81931008    0.022497
4  LoRA Ablation     8  53.608899            147456      82060032    0.179693
5  LoRA Ablation    64  53.528660           1179648      83092224    1.419685
Saved /content/drive/MyDrive/FINETUNE_RES/generations.txt
Saved /content/drive/MyDrive/FINETUNE_RES/ppl_comparison.png
Saved /content/drive/MyDrive/FINETUNE_RES/ablation_curve.png

âœ… All experiments completed. Results saved to: /content/drive/MyDrive/FINETUNE_RES

<Figure size 1000x600 with 1 Axes>
<Figure size 1000x600 with 1 Axes>

