{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18d0e8b0",
   "metadata": {},
   "source": [
    "\n",
    "# \ud83e\uddea Lab 1 \u2014 Generative Models Foundations: **GAN vs VAE** (PyTorch, MNIST)\n",
    "\n",
    "**Course:** Generative AI (Day 1)  \n",
    "**Lab Length:** ~3 hours  \n",
    "**Goal:** Implement a **GAN** and a **VAE** on MNISTFashion-MNIST, compare their behavior, and reflect on stability vs. latent structure.\n",
    "\n",
    "> \u2705 **What you will submit:**  \n",
    "> - A short reflection with generated image grids and comments.\n",
    "\n",
    "### \ud83d\udea6 Rules\n",
    "- Keep training epochs small if you\u2019re on CPU; you can always re-run with more epochs later.\n",
    "- Cells marked **(Provided)** can be run as-is; cells marked **(TODO)** require your edits.\n",
    "\n",
    "### \ud83e\uddf0 Requirements\n",
    "- PyTorch, Torchvision, Matplotlib, TQDM\n",
    "- (Optional) SciPy for a simple FID-like metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9eb12a",
   "metadata": {},
   "source": [
    "\n",
    "## \ud83c\udfaf Learning Objectives\n",
    "- Implement a **vanilla GAN**: Generator + Discriminator, adversarial loss, and training loop.\n",
    "- Implement a **Variational Autoencoder (VAE)**: encoder/decoder, **reparameterization trick**, and **ELBO**.\n",
    "- Produce **visualizations**: sample grids, reconstructions, and **latent interpolations**.\n",
    "- Compare GAN vs VAE using a **proxy FID-like** feature distance.\n",
    "- Reflect on stability, mode collapse, and smoothness of latent space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79827b67",
   "metadata": {},
   "source": [
    "\n",
    "## \ud83e\udded Tips for Success\n",
    "- Use **[-1, 1]** input range for GANs (Tanh output).  \n",
    "- Start with **small networks**. You can always scale up.  \n",
    "- Inspect **loss curves**, generated images, and reconstructions **frequently**.\n",
    "- If your GAN collapses, try: smaller LR, label smoothing, or tweak BatchNorm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00711ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== (Provided) Setup & Installs =====\n",
    "# If you're in Colab, uncomment the next line to ensure dependencies:\n",
    "# !pip -q install torch torchvision torchaudio matplotlib tqdm scipy\n",
    "\n",
    "import math, os, random, numpy as np, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf88852",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== (Provided) Helper: visualization =====\n",
    "def show_grid(tensor, title=\"\", nrow=4, value_range=(-1,1)):\n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, value_range=value_range)\n",
    "    plt.figure(figsize=(4,4)); plt.axis('off'); plt.title(title)\n",
    "    plt.imshow(grid.permute(1,2,0)); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94c25fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== (TODO) Data: MNIST or Fashion-MNIST =====\n",
    "# Hints:\n",
    "# - Normalize to mean=0.5, std=0.5 to map inputs to [-1, 1] for GAN (Tanh output)\n",
    "# - Use batch size around 128 if you have a GPU, smaller if on CPU\n",
    "\n",
    "BATCH = 128  # TODO: adjust if needed\n",
    "use_fashion = True  # TODO: set to True to try Fashion-MNIST\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "if use_fashion:\n",
    "    train_ds = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_ds  = datasets.FashionMNIST(root='./data',  train=False, download=True, transform=transform)\n",
    "else:\n",
    "    train_ds = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_ds  = datasets.MNIST(root='./data',  train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# Quick sanity-check visualization\n",
    "xb, yb = next(iter(train_loader))\n",
    "show_grid(xb[:16], title=\"Real samples (normalized to [-1,1])\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b627703c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Part 1 \u2014 **GAN** (Vanilla)  *(~90 minutes)*\n",
    "\n",
    "We will implement a simple GAN (DCGAN-ish) with:\n",
    "- **Generator**: maps `z ~ N(0, I)` to image `x\u0302`\n",
    "- **Discriminator**: scores real vs fake\n",
    "- **Loss**: Hinge (recommended) **or** BCE (your choice)\n",
    "\n",
    "> **Milestones**\n",
    "> 1) Implement **Generator** & **Discriminator**  \n",
    "> 2) Choose **loss** (hinge recommended), set **optimizers**  \n",
    "> 3) Implement **training loop** and generate sample grids every epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc52424",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== (TODO) GAN Architectures =====\n",
    "# Hints:\n",
    "# - Use Tanh output for G (inputs are normalized to [-1, 1])\n",
    "# - Use LeakyReLU in D; consider BatchNorm in G (not always in D)\n",
    "# - Start small: upsample from (z_dim) -> (128*7*7) -> ConvTranspose to 14x14 -> 28x28\n",
    "# - Keep IMG_CH = 1 for MNIST\n",
    "\n",
    "Z_DIM  = 64   # TODO: try 32, 128 to see effect\n",
    "IMG_CH = 1\n",
    "IMG_H  = 28\n",
    "IMG_W  = 28\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=Z_DIM, img_ch=IMG_CH):\n",
    "        super().__init__()\n",
    "        # Suggested skeleton:\n",
    "        # - Linear(z_dim -> 128*7*7) + BN + ReLU\n",
    "        # - Unflatten to (128, 7, 7)\n",
    "        # - ConvTranspose2d(128 -> 64, kernel=4, stride=2, padding=1) + BN + ReLU  # (64, 14, 14)\n",
    "        # - ConvTranspose2d(64 -> 32, kernel=4, stride=2, padding=1) + BN + ReLU   # (32, 28, 28)\n",
    "        # - Conv2d(32 -> 1, kernel=3, stride=1, padding=1) -> Tanh\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(z_dim, 128*7*7),\n",
    "            nn.BatchNorm1d(128*7*7),\n",
    "            nn.ReLU(True),\n",
    "            nn.Unflatten(1, (128, 7, 7)),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, img_ch, 3, 1, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_ch=IMG_CH):\n",
    "        super().__init__()\n",
    "        # Suggested skeleton:\n",
    "        # - Conv2d(1 -> 32, 4, 2, 1) + LeakyReLU\n",
    "        # - Conv2d(32 -> 64, 4, 2, 1) + BN + LeakyReLU\n",
    "        # - Conv2d(64 -> 128, 3, 2, 1) + LeakyReLU\n",
    "        # - Flatten -> Linear(128*4*4 -> 1)\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(img_ch, 32, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(64, 128, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*4*4, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        f = self.features(x)\n",
    "        logits = self.classifier(f).squeeze(1)\n",
    "        return logits, f\n",
    "\n",
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "# Quick shape tests\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(2, Z_DIM, device=device)\n",
    "    x_fake = G(z)\n",
    "    logit, f = D(x_fake)\n",
    "    assert x_fake.shape == (2, 1, 28, 28), f\"Got {x_fake.shape}\"\n",
    "    assert logit.shape[0] == 2, f\"Got {logit.shape}\"\n",
    "print(\"\u2713 GAN shapes look OK.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe56a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== (TODO) GAN Losses =====\n",
    "# Option A: Hinge loss (recommended)\n",
    "def d_loss_hinge(real_logits, fake_logits):\n",
    "    # implement hinge: E[max(0, 1 - D(real))] + E[max(0, 1 + D(fake))]\n",
    "    return F.relu(1.0 - real_logits).mean() + F.relu(1.0 + fake_logits).mean()\n",
    "\n",
    "def g_loss_hinge(fake_logits):\n",
    "    # implement generator hinge: -E[D(fake)]\n",
    "    return -fake_logits.mean()\n",
    "\n",
    "# Option B: BCE \n",
    "# TODO: To be tested\n",
    "# bce = nn.BCEWithLogitsLoss()\n",
    "# def d_loss_bce(real_logits, fake_logits):\n",
    "#     real_t = torch.ones_like(real_logits)\n",
    "#     fake_t = torch.zeros_like(fake_logits)\n",
    "#     return bce(real_logits, real_t) + bce(fake_logits, fake_t)\n",
    "# def g_loss_bce(fake_logits):\n",
    "#     real_t = torch.ones_like(fake_logits)\n",
    "#     return bce(fake_logits, real_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04082ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== (TODO) GAN Training =====\n",
    "# Hints:\n",
    "# - Alternate D then G updates\n",
    "# - Sample fresh z for each update\n",
    "# - Visualize fixed z grid per epoch\n",
    "LR = 2e-4\n",
    "betas = (0.5, 0.999)\n",
    "opt_G = torch.optim.Adam(G.parameters(), lr=LR, betas=betas)\n",
    "opt_D = torch.optim.Adam(D.parameters(), lr=LR, betas=betas)\n",
    "\n",
    "EPOCHS_GAN = 5  # Increase if you have GPU time\n",
    "fixed_z = torch.randn(16, Z_DIM, device=device)\n",
    "\n",
    "for epoch in range(1, EPOCHS_GAN+1):\n",
    "    G.train(); D.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"[GAN] Epoch {epoch}/{EPOCHS_GAN}\")\n",
    "    for x, _ in pbar:\n",
    "        x = x.to(device)\n",
    "\n",
    "        # (1) Update D\n",
    "        z = torch.randn(x.size(0), Z_DIM, device=device)\n",
    "        with torch.no_grad():\n",
    "            x_fake = G(z)\n",
    "        real_logits, _ = D(x)\n",
    "        fake_logits, _ = D(x_fake)\n",
    "        loss_D = d_loss_hinge(real_logits, fake_logits)  # or d_loss_bce(...)\n",
    "        opt_D.zero_grad(set_to_none=True)\n",
    "        loss_D.backward()\n",
    "        opt_D.step()\n",
    "\n",
    "        # (2) Update G\n",
    "        z = torch.randn(x.size(0), Z_DIM, device=device)\n",
    "        x_fake = G(z)\n",
    "        fake_logits, _ = D(x_fake)\n",
    "        loss_G = g_loss_hinge(fake_logits)  # or g_loss_bce(...)\n",
    "        opt_G.zero_grad(set_to_none=True)\n",
    "        loss_G.backward()\n",
    "        opt_G.step()\n",
    "\n",
    "        pbar.set_postfix({'D': f\"{loss_D.item():.3f}\", 'G': f\"{loss_G.item():.3f}\"})\n",
    "\n",
    "    with torch.no_grad():\n",
    "        samples = G(fixed_z).cpu()\n",
    "    show_grid(samples, title=f\"GAN samples (epoch {epoch})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e3f4f0",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Part 2 \u2014 **VAE**  *(~60 minutes)*\n",
    "\n",
    "We will implement:\n",
    "- Encoder that outputs **\u03bc** and **log \u03c3\u00b2**\n",
    "- **Reparameterization trick**: `z = \u03bc + \u03c3 \u2299 \u03b5`\n",
    "- Decoder that reconstructs `x\u0302`\n",
    "- **ELBO** loss = reconstruction + KL divergence\n",
    "\n",
    "> **Milestones**\n",
    "> 1) Build **VAE module** (encode/reparameterize/decode)  \n",
    "> 2) Implement **loss** (reconstruction + KL)  \n",
    "> 3) Train and visualize reconstructions & random samples  \n",
    "> 4) Do a **latent interpolation** between two test images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97915b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== (TODO) VAE Architecture =====\n",
    "LATENT = 16  # Try 8, 16, 32 to see effects\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent=LATENT):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 4, 2, 1),   # 14x14\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1),  # 7x7\n",
    "            nn.ReLU(True),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.enc_fc_mu  = nn.Linear(64*7*7, latent)\n",
    "        self.enc_fc_log = nn.Linear(64*7*7, latent)\n",
    "\n",
    "        # Decoder\n",
    "        self.dec_fc = nn.Linear(latent, 64*7*7)\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Unflatten(1, (64, 7, 7)),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),  # 14x14\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 16, 4, 2, 1),  # 28x28\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(16, 1, 3, 1, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.enc(x)\n",
    "        mu = self.enc_fc_mu(h)\n",
    "        logvar = self.enc_fc_log(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = (0.5*logvar).exp()\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.dec_fc(z)\n",
    "        x = self.dec(h)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        xhat = self.decode(z)\n",
    "        return xhat, mu, logvar\n",
    "\n",
    "vae = VAE().to(device)\n",
    "\n",
    "# Quick shape tests\n",
    "with torch.no_grad():\n",
    "    x = xb[:2].to(device)\n",
    "    xhat, mu, logvar = vae(x)\n",
    "    assert xhat.shape == x.shape, f\"{xhat.shape} vs {x.shape}\"\n",
    "    assert mu.shape[-1] == LATENT and logvar.shape[-1] == LATENT\n",
    "print(\"\u2713 VAE shapes look OK.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074625ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== (TODO) VAE Loss & Training =====\n",
    "# Hint: ELBO \u2248 recon_loss + KL(q(z|x) || p(z)), with p(z)=N(0,I)\n",
    "# - Use L1 or BCE for reconstruction (L1 often looks nicer on MNIST)\n",
    "# - KL term: -0.5 * sum(1 + logvar - mu^2 - exp(logvar))\n",
    "\n",
    "def vae_loss(xhat, x, mu, logvar):\n",
    "    recon = F.l1_loss(xhat, x, reduction='sum') / x.size(0)  # try also BCE\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)\n",
    "    return recon + kl, recon, kl\n",
    "\n",
    "opt_vae = torch.optim.Adam(vae.parameters(), lr=2e-3)\n",
    "\n",
    "EPOCHS_VAE = 5  # Increase if you have GPU time\n",
    "for epoch in range(1, EPOCHS_VAE+1):\n",
    "    vae.train()\n",
    "    losses = []\n",
    "    pbar = tqdm(train_loader, desc=f\"[VAE] Epoch {epoch}/{EPOCHS_VAE}\")\n",
    "    for x, _ in pbar:\n",
    "        x = x.to(device)\n",
    "        xhat, mu, logvar = vae(x)\n",
    "        loss, rec, kl = vae_loss(xhat, x, mu, logvar)\n",
    "        opt_vae.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt_vae.step()\n",
    "        losses.append(loss.item())\n",
    "        pbar.set_postfix({'loss': f\"{np.mean(losses):.2f}\"})\n",
    "    # visualize reconstructions\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        x = xb[:16].to(device)\n",
    "        xhat, _, _ = vae(x)\n",
    "    show_grid(x.cpu(), title=\"VAE inputs\")\n",
    "    show_grid(xhat.cpu(), title=f\"VAE reconstructions (epoch {epoch})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ac19a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== (TODO) Sampling & Latent Interpolation =====\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(16, LATENT, device=device)\n",
    "    samples = vae.decode(z).cpu()\n",
    "show_grid(samples, title=\"VAE random samples\")\n",
    "\n",
    "# Latent interpolation between two test images\n",
    "def interpolate(a, b, steps=8):\n",
    "    alphas = torch.linspace(0, 1, steps, device=a.device).view(-1,1)\n",
    "    return (1-alphas)*a + alphas*b\n",
    "\n",
    "with torch.no_grad():\n",
    "    x, _ = next(iter(test_loader))\n",
    "    x = x.to(device)[:2]\n",
    "    mu, logvar = vae.encode(x)\n",
    "    z1 = mu[0]; z2 = mu[1]\n",
    "    z_traj = interpolate(z1, z2, steps=16)\n",
    "    interp_imgs = vae.decode(z_traj).cpu()\n",
    "show_grid(interp_imgs, title=\"VAE latent interpolation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6adcafd",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Part 3 \u2014 **Comparison & Proxy FID-like Metric**  *(~30 minutes)*\n",
    "\n",
    "We will compare samples from the GAN and VAE using a **feature Fr\u00e9chet distance** proxy:\n",
    "1) Extract features from the **Discriminator** (penultimate conv layer)\n",
    "2) Fit Gaussians to **real** vs **fake** features\n",
    "3) Compute **Fr\u00e9chet distance**:  \n",
    "   $\\|\\mu_r-\\mu_f\\|^2 + \\mathrm{Tr}(\\Sigma_r + \\Sigma_f - 2(\\Sigma_r \\Sigma_f)^{1/2})$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> This is not the official FID (which uses Inception), but behaves similarly for quick lab work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d36c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== (Optional TODO) Proxy FID-like Metric =====\n",
    "# Requires scipy for sqrtm\n",
    "try:\n",
    "    from scipy import linalg\n",
    "    SCIPY_OK = True\n",
    "except Exception:\n",
    "    SCIPY_OK = False\n",
    "    print(\"SciPy not available \u2014 skipping proxy FID. You can !pip install scipy and re-run.\")\n",
    "\n",
    "def get_features(disc, loader, n_batches=50, use_fake=False, generator=None):\n",
    "    disc.eval()\n",
    "    feats = []\n",
    "    with torch.no_grad():\n",
    "        for i, (x, _) in enumerate(loader):\n",
    "            if i >= n_batches: break\n",
    "            x = x.to(device)\n",
    "            if use_fake:\n",
    "                z = torch.randn(x.size(0), Z_DIM, device=device)\n",
    "                x = generator(z)\n",
    "            _, f = disc(x)\n",
    "            f = F.adaptive_avg_pool2d(f, 1).flatten(1)  # (B, C)\n",
    "            feats.append(f.cpu())\n",
    "    return torch.cat(feats, dim=0).numpy()\n",
    "\n",
    "def gaussian_stats(X):\n",
    "    mu = X.mean(axis=0)\n",
    "    sigma = np.cov(X, rowvar=False)\n",
    "    return mu, sigma\n",
    "\n",
    "def frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    diff = mu1 - mu2\n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    if not np.isfinite(covmean).all():\n",
    "        covmean = linalg.sqrtm((sigma1 + np.eye(sigma1.shape[0])*eps).dot(sigma2 + np.eye(sigma2.shape[0])*eps))\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2*covmean)\n",
    "    return float(fid)\n",
    "\n",
    "if SCIPY_OK:\n",
    "    print(\"Computing real features...\")\n",
    "    real_feats = get_features(D, test_loader, n_batches=80, use_fake=False)\n",
    "    print(\"Computing GAN fake features...\")\n",
    "    fake_feats_gan = get_features(D, test_loader, n_batches=80, use_fake=True, generator=G)\n",
    "    mu_r, sig_r = gaussian_stats(real_feats)\n",
    "    mu_g, sig_g = gaussian_stats(fake_feats_gan)\n",
    "    fid_gan = frechet_distance(mu_r, sig_r, mu_g, sig_g)\n",
    "    print(f\"Proxy FID (GAN vs real): {fid_gan:.2f}\")\n",
    "\n",
    "    # VAE samples\n",
    "    vae.eval()\n",
    "    all_vae = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(80):\n",
    "            z = torch.randn(BATCH, LATENT, device=device)\n",
    "            all_vae.append(vae.decode(z).cpu())\n",
    "    all_vae = torch.cat(all_vae, dim=0)[:len(real_feats)]\n",
    "\n",
    "    fake_feats_vae = []\n",
    "    print(\"Computing VAE fake features...\")\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(all_vae), BATCH):\n",
    "            batch = all_vae[i:i+BATCH].to(device)\n",
    "            _, f = D(batch)\n",
    "            f = F.adaptive_avg_pool2d(f, 1).flatten(1)\n",
    "            fake_feats_vae.append(f.cpu())\n",
    "    fake_feats_vae = torch.cat(fake_feats_vae, dim=0).numpy()\n",
    "    mu_v, sig_v = gaussian_stats(fake_feats_vae)\n",
    "    fid_vae = frechet_distance(mu_r, sig_r, mu_v, sig_v)\n",
    "    print(f\"Proxy FID (VAE vs real): {fid_vae:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870d87ac",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcdd Final Reflection (to submit)\n",
    "\n",
    "1. Copy all the generated outputs, don't forget to label them (e.g   Fashion-MNIST, GAN, Z_DIM=128, EPOCH=... )\n",
    "\n",
    "2. Include image grids:\n",
    "   - GAN samples (best epoch)\n",
    "   - VAE reconstructions\n",
    "   - VAE latent interpolation\n",
    "\n",
    "3. Include your **proxy FID-like** numbers for GAN and VAE.\n",
    "\n",
    "4. Answer briefly:\n",
    "   - What hyperparameters most influenced **GAN stability** in your runs?\n",
    "   - Evidence of **mode collapse** (if any)? What helped?\n",
    "   - How did **latent dim** affect VAE reconstructions and samples?\n",
    "   - One idea to combine benefits of both models (e.g., **VAE-GAN**).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf16395",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab 1 \u2014 Generative Models Foundations: GAN vs VAE (PyTorch, MNIST) \u2014 Student Notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}