{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "193fece8",
   "metadata": {},
   "source": [
    "\n",
    "# \ud83e\uddea Lab 2 \u2014 **LLM Fine-tuning** (DistilGPT2) + **LoRA** + **Prompting**\n",
    "\n",
    "**Course:** Generative AI (Day 2)  \n",
    "**Lab Length:** ~3 hours\n",
    "\n",
    "**Goal:** Fine-tune a small causal LM on a tiny domain corpus (Tiny Shakespeare), compare **full fine-tuning** vs **LoRA**, and experiment with **prompt engineering**.\n",
    "\n",
    "> \u2705 **Deliverables (submit this notebook):**\n",
    "> - Baseline vs fine-tuned **perplexity** table\n",
    "> - 3\u20135 **generations** for the same prompts across models (baseline / full FT / LoRA)\n",
    "> - Short **prompting analysis** (zero-shot, few-shot, instruction template)\n",
    "> - 3\u20135 bullet **takeaways**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61c591a",
   "metadata": {},
   "source": [
    "\n",
    "### \ud83d\udea6 Rules & Guidance\n",
    "- If `bitsandbytes` isn't available, LoRA still works without 4-bit (skip QLoRA).\n",
    "- Cells marked **(Provided)** can be run as-is; **(TODO)** require your edits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d158f5",
   "metadata": {},
   "source": [
    "\n",
    "## \ud83c\udfaf Learning Objectives\n",
    "- Prepare data, tokenize, and **pack** sequences for causal LM training.\n",
    "- Compute baseline **validation perplexity** (no fine-tuning).\n",
    "- Run a short **full fine-tune** (a few epochs) and evaluate.\n",
    "- Apply **LoRA (PEFT)** (optionally QLoRA) and evaluate.\n",
    "- Compare **generations** and analyze **prompting** strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef012df",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U triton\n",
    "\n",
    "# ===== (Provided) Setup & Installs =====\n",
    "# If you're in Colab, uncomment to install pinned versions:\n",
    "!pip -q install transformers==4.44.2 datasets==2.20.0 accelerate==0.34.2 peft==0.12.0 bitsandbytes==0.43.1 evaluate==0.4.2 rouge-score -U\n",
    "\n",
    "import os, math, random, numpy as np, torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling,\n",
    "                          Trainer, TrainingArguments, set_seed)\n",
    "from transformers import pipeline\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import evaluate\n",
    "\n",
    "set_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fc0163",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Part 0 \u2014 Data (Tiny Shakespeare) *(~10 min)*\n",
    "\n",
    "We'll use the **Tiny Shakespeare** dataset (~1MB).  \n",
    "Feel free to swap with a small domain corpus if desired.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffd6a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== (Provided) Load dataset =====\n",
    "raw = load_dataset(\"Trelis/tiny-shakespeare\")\n",
    "# Split into train/valid/test quickly\n",
    "split = raw[\"train\"].train_test_split(test_size=0.02, seed=42)\n",
    "test_valid = split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "dataset = DatasetDict({\n",
    "    \"train\": split[\"train\"],\n",
    "    \"validation\": test_valid[\"train\"],\n",
    "    \"test\": test_valid[\"test\"]\n",
    "})\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbb9bbd",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Part 1 \u2014 Tokenization & Packing *(~25\u201330 min)*\n",
    "\n",
    "**Tasks:**\n",
    "1) Load tokenizer for `distilgpt2` and set `pad_token = eos_token`.  \n",
    "2) **Tokenize** the text.  \n",
    "3) **Group/pack** into fixed-length blocks (e.g., `block_size = 256`) with `labels = input_ids`.\n",
    "\n",
    "> \ud83d\udca1 **Hints**\n",
    "> - Use `remove_columns=[\"text\"]` in `map` after tokenization.\n",
    "> - For grouping, concatenate token lists and then slice into blocks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40f717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== (TODO) Tokenizer & packing =====\n",
    "model_id = \"distilgpt2\"\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "tok.pad_token = tok.eos_token  # important for collation\n",
    "block_size = 256             \n",
    "\n",
    "def tokenize(ex):\n",
    "    out = tok(ex[\"Text\"])\n",
    "    return out\n",
    "\n",
    "tokenized = dataset.map(tokenize, batched=True, remove_columns=[\"Text\"])\n",
    "tokenized\n",
    "\n",
    "def group_texts(examples):\n",
    "    # - Concatenate lists per key\n",
    "    # - Truncate to a multiple of block_size\n",
    "    # - Split into fixed blocks\n",
    "    concat = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_len = (len(concat[\"input_ids\"]) // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i:i+block_size] for i in range(0, total_len, block_size)]\n",
    "        for k, t in concat.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized.map(group_texts, batched=True)\n",
    "lm_datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087de52d",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Part 2 \u2014 Baseline Perplexity *(~20 min)*\n",
    "\n",
    "Compute validation **perplexity** before any fine-tuning.\n",
    "\n",
    "> \ud83d\udca1 **Hints**\n",
    "> - Use `AutoModelForCausalLM` with `distilgpt2`.\n",
    "> - Use `DataCollatorForLanguageModeling(mlm=False)` for causal LM.\n",
    "> - Evaluate a limited number of batches (e.g., 50) for speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bd6c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== Baseline perplexity =====\n",
    "baseline_model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "collator = DataCollatorForLanguageModeling(tok, mlm=False)\n",
    "\n",
    "def compute_ppl(model, ds, max_batches=50):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    loader = torch.utils.data.DataLoader(ds, batch_size=8, shuffle=False, collate_fn=collator)\n",
    "    for i, batch in enumerate(loader):\n",
    "        if i >= max_batches: break\n",
    "        for k in batch:\n",
    "            batch[k] = batch[k].to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(**batch)\n",
    "            loss = out.loss.detach().float()\n",
    "        losses.append(loss.item())\n",
    "    mean_loss = float(np.mean(losses)) if losses else float(\"inf\")\n",
    "    return math.exp(mean_loss) if mean_loss < 20 else float(\"inf\")\n",
    "\n",
    "ppl_baseline = compute_ppl(baseline_model, lm_datasets[\"validation\"])\n",
    "print(f\"Baseline validation PPL: {ppl_baseline:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9f9ad3",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Part 3 \u2014 Full Fine-tune *(~45\u201360 min)*\n",
    "\n",
    "Run a short fine-tune (2\u20133 epochs).\n",
    "\n",
    "> \ud83d\udca1 **Hints**\n",
    "> - Start with `per_device_train_batch_size=8`, `grad_accum_steps=2` (adjust to VRAM).\n",
    "> - `fp16=True` if on GPU.\n",
    "> - Log with `logging_steps=50` and evaluate per epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe382b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== Full fine-tune =====\n",
    "ft_model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./ft-distilgpt2\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=5,      # (TODO) increase if time allows\n",
    "    warmup_ratio=0.03,\n",
    "    weight_decay=0.1,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=ft_model,\n",
    "    args=args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"validation\"],\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "ppl_ft = compute_ppl(ft_model, lm_datasets[\"validation\"])\n",
    "print(f\"Full FT validation PPL: {ppl_ft:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b51d7c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Part 4 \u2014 Quick Generations (Full FT) *(~10 min)*\n",
    "\n",
    "Generate a short sample with your **fine-tuned** model for qualitative inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b5e73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== (Provided) Generation helper for FT model =====\n",
    "gen_ft = pipeline(\"text-generation\", model=ft_model, tokenizer=tok, device=0 if device==\"cuda\" else -1)\n",
    "\n",
    "prompt = \"ROMEO: I dreamt tonight that\"\n",
    "out = gen_ft(prompt, max_new_tokens=80, do_sample=True, temperature=0.9, top_p=0.95)[0][\"generated_text\"]\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3eea9c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Part 5 \u2014 LoRA (PEFT) *(~45 min)*\n",
    "\n",
    "Train with **LoRA** (and optionally QLoRA if 4-bit quantization is available).\n",
    "\n",
    "> \ud83d\udca1 **Hints**\n",
    "> - If `bitsandbytes` is available, use `BitsAndBytesConfig(load_in_4bit=True)` + `prepare_model_for_kbit_training`.\n",
    "> - Set `r=8..16`, `lora_alpha=16..32`, `lora_dropout\u22480.05`.\n",
    "> - Compare PPL vs full FT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7454e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== LoRA fine-tune =====\n",
    "# NOTE: We have disabled 4-bit quantization (bitsandbytes) to avoid Colab errors.\n",
    "# DistilGPT2 is small enough to run fine without it.\n",
    "\n",
    "base_for_lora = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16, lora_alpha=32, lora_dropout=0.05,\n",
    "    target_modules=[\"c_attn\", \"q_attn\", \"v_attn\"] if \"gpt2\" in model_id else None,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "lora_model = get_peft_model(base_for_lora, lora_cfg)\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "args_lora = TrainingArguments(\n",
    "    output_dir=\"./lora-distilgpt2\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    learning_rate=1.5e-4,\n",
    "    num_train_epochs=2,\n",
    "    warmup_ratio=0.03,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer_lora = Trainer(\n",
    "    model=lora_model,\n",
    "    args=args_lora,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"validation\"],\n",
    "    data_collator=collator,\n",
    ")\n",
    "trainer_lora.train()\n",
    "\n",
    "ppl_lora = compute_ppl(lora_model, lm_datasets[\"validation\"])\n",
    "print(f\"LoRA validation PPL: {ppl_lora:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e7415e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Part 6 \u2014 Prompting Experiments *(~30 min)*\n",
    "\n",
    "Run **the same prompts** through:\n",
    "- Baseline (no FT)\n",
    "- Full FT\n",
    "- LoRA\n",
    "\n",
    "Explore:\n",
    "- **Zero-shot** continuation\n",
    "- **Instruction-style** template (even if GPT-2 family isn't instruction-tuned, observe behavior)\n",
    "- **Few-shot** pattern completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a665551",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== Compare generations across models =====\n",
    "gen_base = pipeline(\"text-generation\", model=baseline_model, tokenizer=tok, device=0 if device==\"cuda\" else -1)\n",
    "gen_lora = pipeline(\"text-generation\", model=lora_model, tokenizer=tok, device=0 if device==\"cuda\" else -1)\n",
    "\n",
    "prompts = [\n",
    "    \"ROMEO: I dreamt tonight that\",\n",
    "    \"### Instruction:\\nRewrite the line in modern English.\\n### Input:\\n'Thou art more lovely and more temperate.'\\n### Response:\\n\",\n",
    "    \"SCENE PROMPTS:\\nExample: 'A dark forest at midnight.' -> 'The moon hides as branches claw at the sky.'\\n\"\n",
    "    \"Example: 'A crowded marketplace.' -> 'Vendors roar; coins clatter like rain.'\\n\"\n",
    "    \"Task: 'A stormy seashore.' -> \"\n",
    "]\n",
    "\n",
    "def run_batch(gen_pipe, name):\n",
    "    print(f\\\"\\n=== {name} ===\\\")\n",
    "    for p in prompts:\n",
    "        out = gen_pipe(p, max_new_tokens=60, do_sample=True, temperature=0.9, top_p=0.92)[0][\\\"generated_text\\\"]\n",
    "        print(f\\\"\\n[Prompt]\\n{p}\\n[Output]\\n{out}\\n{'-'*60}\\\")\n",
    "\n",
    "run_batch(gen_base, \"Baseline\")\n",
    "run_batch(gen_ft,   \"Full FT\")\n",
    "run_batch(gen_lora, \"LoRA\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216b938f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Part 7 \u2014 Challenge: The \"Catastrophic Forgetting\" Test\n",
    "\n",
    "**Context:** You have fine-tuned the model to speak like Shakespeare. But what happened to the general knowledge it had before? \n",
    "\n",
    "When we aggressively fine-tune a small model on a narrow dataset, we often trigger **Catastrophic Forgetting**\u2014the model \"forgets\" how to speak modern English or answer factual questions because its weights have been overwritten to minimize loss on Shakespeare plays.\n",
    "\n",
    "**Task:**\n",
    "1. Run the code below using a **modern** prompt (something that definitely didn't exist in Shakespeare's time).\n",
    "2. Observe how the **Full Fine-Tune** model struggles compared to the **Baseline**.\n",
    "3. **Crucial:** Analyze if **LoRA** preserves more general knowledge than Full FT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8975d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== (TODO) The Forgetting Test =====\n",
    "modern_prompts = [\n",
    "    \"The capital of France is\",\n",
    "    \"To install a python package, you should run\",\n",
    "    \"The theory of relativity was proposed by\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== Forgetting Test: Modern Prompts ===\\n\")\n",
    "\n",
    "for p in modern_prompts:\n",
    "    print(f\"\\n>>> Prompt: {p}\")\n",
    "    # Baseline\n",
    "    try:\n",
    "        out_base = gen_base(p, max_new_tokens=30, do_sample=True, temperature=0.5)[0][\"generated_text\"]\n",
    "        print(f\"[Baseline]: {out_base[len(p):].strip()} ...\") \n",
    "    except NameError:\n",
    "        print(\"[Baseline]: Model pipeline not found\")\n",
    "\n",
    "    # Full FT\n",
    "    try:\n",
    "        out_ft = gen_ft(p, max_new_tokens=30, do_sample=True, temperature=0.5)[0][\"generated_text\"]\n",
    "        print(f\"[Full FT]:  {out_ft[len(p):].strip()} ...\")\n",
    "    except NameError:\n",
    "        print(\"[Full FT]:  Model pipeline not found\")\n",
    "\n",
    "    # LoRA\n",
    "    try:\n",
    "        out_lora = gen_lora(p, max_new_tokens=30, do_sample=True, temperature=0.5)[0][\"generated_text\"]\n",
    "        print(f\"[LoRA]:     {out_lora[len(p):].strip()} ...\")\n",
    "    except NameError:\n",
    "        print(\"[LoRA]:     Model pipeline not found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf3d586",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 8 \u2014 The \"Ablation Study\" \n",
    "\n",
    "**Warning: This section requires significant compute time.**\n",
    "\n",
    "**Context:**\n",
    "In Part 5, you used a LoRA Rank (`r`) of **16**. But why 16? Why not 1? Why not 100?\n",
    "In Deep Learning, hyperparameters are rarely guessed correctly on the first try. We must perform an **Ablation Study**\u2014a systematic experiment where we change one variable to see its impact.\n",
    "\n",
    "**The Theory:**\n",
    "* **Rank (`r`)** determines the size of the low-rank matrices.\n",
    "* **High Rank (e.g., 64):** More trainable parameters. Theoretically \"smarter,\" but slower to train and higher risk of overfitting.\n",
    "* **Low Rank (e.g., 1 or 8):** Fewer parameters. Faster, but might not have the \"capacity\" to learn the style.\n",
    "\n",
    "**Your Task:**\n",
    "You will run the LoRA training loop **3 distinct times** to find the \"Sweet Spot.\"\n",
    "\n",
    "1. **Run A:** `r = 1` (Extreme compression)\n",
    "2. **Run B:** `r = 8` (Low capacity)\n",
    "3. **Run C:** `r = 64` (High capacity)\n",
    "\n",
    "*Note: To save time, you can reduce `num_train_epochs` to **1** for these experiments, as we are looking for relative differences, not absolute convergence.*\n",
    "\n",
    "### \ud83d\udcdd Deliverable Table (Fill this out)\n",
    "\n",
    "| Experiment | Rank (`r`) | Trainable Parameters (%) | Validation PPL | Did it learn the style? (Subjective) |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **Run A** | 1 | ... % | ... | ... |\n",
    "| **Run B** | 8 | ... % | ... | ... |\n",
    "| **Run C** | 64 | ... % | ... | ... |\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "*(Run the code cell below to execute the loop. Go grab a coffee, this will take a while!)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Memory Cleanup before Ablation =====\n",
    "import gc\n",
    "print(\"Cleaning up memory...\")\n",
    "try:\n",
    "    del baseline_model\n",
    "    del ft_model\n",
    "    del lora_model\n",
    "    del base_for_lora\n",
    "    del trainer\n",
    "    del trainer_lora\n",
    "except NameError:\n",
    "    pass\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Memory cleared.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832e913c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Ablation Study Loop =====\n",
    "# This cell runs the training 3 times. \n",
    "# WARNING: This will take significant time. \n",
    "\n",
    "ranks_to_test = [1, 8, 64]\n",
    "results = {}\n",
    "\n",
    "for r_val in ranks_to_test:\n",
    "    print(f\"\\n\\n{'='*20} STARTING TRAINING WITH RANK: {r_val} {'='*20}\")\n",
    "    \n",
    "    # 1. Config with new Rank\n",
    "    # Note: We scale alpha to match r roughly, or keep it fixed. \n",
    "    # For this experiment, we will keep alpha fixed at 32 to isolate 'r'.\n",
    "    loop_config = LoraConfig(\n",
    "        r=r_val, \n",
    "        lora_alpha=32, \n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"c_attn\", \"q_attn\", \"v_attn\"], \n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    \n",
    "    # 2. Prepare Model\n",
    "    model_to_train = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "    # (Add quantization logic here if using QLoRA, otherwise standard:)\n",
    "    loop_model = get_peft_model(model_to_train, loop_config)\n",
    "    \n",
    "    # Print param %\n",
    "    trainable, total = loop_model.get_nb_trainable_parameters()\n",
    "    print(f\"Trainable params: {trainable} || {100 * trainable / total:.4f}%\")\n",
    "    \n",
    "    # 3. Training Arguments (Unique output dir is crucial!)\n",
    "    loop_args = TrainingArguments(\n",
    "        output_dir=f\"./lora-rank-{r_val}\",\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=2,\n",
    "        learning_rate=1.5e-4,\n",
    "        num_train_epochs=1,      # Reduced to 1 epoch to save YOUR time, typically 2-3\n",
    "        logging_steps=50,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    \n",
    "    # 4. Train\n",
    "    loop_trainer = Trainer(\n",
    "        model=loop_model,\n",
    "        args=loop_args,\n",
    "        train_dataset=lm_datasets[\"train\"],\n",
    "        eval_dataset=lm_datasets[\"validation\"],\n",
    "        data_collator=collator\n",
    "    )\n",
    "    loop_trainer.train()\n",
    "    \n",
    "    # 5. Evaluate\n",
    "    ppl = compute_ppl(loop_model, lm_datasets[\"validation\"])\n",
    "    results[r_val] = ppl\n",
    "    print(f\"Rank {r_val} Finished. PPL: {ppl:.2f}\")\n",
    "    \n",
    "    # Clear memory\n",
    "    del loop_model, loop_trainer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\\n=== FINAL RESULTS ===\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8371420",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcdd Final Report (fill before submission)\n",
    "\n",
    "Complete the table and add brief commentary.\n",
    "\n",
    "### Perplexity\n",
    "| Model | Val PPL |\n",
    "|------|---------|\n",
    "| Baseline (distilgpt2) | \u2026 |\n",
    "| Full Fine-tune | \u2026 |\n",
    "| LoRA | \u2026 |\n",
    "\n",
    "### Generations (same prompts)\n",
    "- Prompt A:  \n",
    "  - Baseline \u2192 \u2026  \n",
    "  - Full FT \u2192 \u2026  \n",
    "  - LoRA \u2192 \u2026  \n",
    "\n",
    "- Prompt B: \u2026\n",
    "\n",
    "### Prompt Engineering Notes\n",
    "- Zero-shot vs few-shot: \u2026  \n",
    "- Instruction template effects: \u2026  \n",
    "- Failure cases / artifacts: \u2026  \n",
    "\n",
    "### Analysis: Catastrophic Forgetting\n",
    "* **Observation:** How did the Full Fine-Tuned model respond to the modern prompt compared to the Baseline? Did it hallucinate Shakespearean words?\n",
    "* **LoRA vs Full FT:** Did the LoRA model retain more \"modern English\" capability than the Full FT model? \n",
    "* **Theory:** Why does LoRA (training <1% of parameters) theoretically help prevent forgetting compared to updating 100% of parameters?\n",
    "\n",
    "### Analysis Questions: The \"Ablation Study\" \n",
    "\n",
    "1. **The \"Diminishing Returns\" Trap:** Did increasing the Rank from 8 to 64 result in a massive drop in Perplexity, or was the improvement marginal?\n",
    "2. **Efficiency:** Look at the file size or parameter count of `r=1` vs `r=64`. Given the performance difference you observed, which Rank would you choose for a production mobile app?\n",
    "3. **Overfitting:** Did the high-rank model (`r=64`) start to memorize the training data (loss goes down) but fail to generalize (validation PPL stays high)? Explain based on your logs.\n",
    "\n",
    "### Takeaways (3\u20135 bullets)\n",
    "- \u2026\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ff2bbe",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab 2 \u2014 LLM Fine-tuning (DistilGPT2) + LoRA + Prompting \u2014 Student Notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}